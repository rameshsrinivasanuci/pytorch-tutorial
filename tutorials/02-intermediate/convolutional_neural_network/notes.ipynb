{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "> Building an training of the convolutional neural network\n",
    ">\n",
    "> **Table of Contents**\n",
    ">> 1. Import Libraries\n",
    ">>\n",
    ">> 2. Load Dataset\n",
    ">>\n",
    ">> 3. Define Model\n",
    ">>\n",
    ">> 4. Configure Device to Execute Model\n",
    ">>\n",
    ">> 5. Define Loss Function and Optimizer\n",
    ">>\n",
    ">> 6. Train Model\n",
    ">>\n",
    ">> 7. Test Model\n",
    ">>\n",
    ">> 8. Save Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import libraries needed for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch for machine learning capabilities\n",
    "import torch\n",
    "\n",
    "#import torchvision for image dataset (MNIST) and image transformations\n",
    "import torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the libraries imported, we will now load the data to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download and reference the training dataset\n",
    "training_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "#Download and reference the testing dataset\n",
    "testing_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                             train=False,\n",
    "                                             transform=torchvision.transforms.ToTensor(),\n",
    "                                             download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After loading the dataset, the next step is to specify the dataload to load the data in batches\n",
    ">\n",
    "> Because data will be being loaded in batches, we will define a hyperparameter for the batch size of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the hyperparameter for the batch size (i.e., chunck size of the data being loaded)\n",
    "batch_size = 100\n",
    "\n",
    "#Define the DataLoader for the training dataset\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=False)\n",
    "\n",
    "#Define the DataLoader for the training dataset\n",
    "testing_loader = torch.utils.data.DataLoader(dataset=testing_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             drop_last=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will now specify a convolutional neural network to take in some input vector, apply convolution operations, and output a probability vector\n",
    ">\n",
    "> When defining the model, we will use the module list (which is a list of module) to define the order of layers and use the sequential list (which is a cascading of modules) to define each convolutional layer\n",
    ">\n",
    "> **There are additional hyperparmeters for the CNN that I do not yet understand such as kernel size and stride. I will need to investigate into them an why there is a difference hen using them for the conv2d and maxpool2d layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Neural Network (CNN) Class\n",
    "class CNN(torch.nn.Module):\n",
    "    #CNN constructor function\n",
    "    def __init__(self, num_classes):\n",
    "        #First, call the constructor of the parent class\n",
    "        super(CNN, self).__init__()\n",
    "        #Specify the module list for the sequence of layers in the CNN\n",
    "        self.module_list = list()\n",
    "        #Define the first convolutional layer\n",
    "        layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        #Append the convolutional layer to the module list\n",
    "        self.module_list.append(layer1)\n",
    "        #Define the second convolutional layer\n",
    "        layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        #Append the convolutional layer to the module list\n",
    "        self.module_list.append(layer2)\n",
    "        #Append the linear layer to the model\n",
    "        self.module_list.append(torch.nn.Linear(7*7*32, num_classes))\n",
    "        #Log-Softmax used for numberical stability\n",
    "        self.module_list.append(torch.nn.LogSoftmax(dim=1))\n",
    "        #Lastly, convert the list object of modules into a ModuleList object (used so that PyTorch can detect the modules/parameters contained in the list)\n",
    "        self.layers = torch.nn.ModuleList(self.module_list)\n",
    "\n",
    "    #Define how the model applies forward propogation\n",
    "    def forward(self, x):\n",
    "        #Initially have the output be the passed data\n",
    "        out = x\n",
    "        #Loop through each layer in the model\n",
    "        for layer in self.layers:\n",
    "            #get the output for each layer, passing it to the next layer\n",
    "            out = layer(out)\n",
    "        #Specify for the output to require the use the gradient for backward propogation\n",
    "        out.require_grad_()\n",
    "        #return the output\n",
    "        return(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the CNN model now defined, we must instantiate the class to then train\n",
    ">\n",
    "> For this model, all we need to specify is the output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameter for the output size\n",
    "num_classes = 10\n",
    "\n",
    "#Instantiate the model\n",
    "model = CNN(num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Device to Execute Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the model defined, we can select the device to train the model on\n",
    ">\n",
    "> We will opt for the GPU via the CUDA library, but if it is unavailable, we will use the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the device for the model based on availability\n",
    "device = torch.device(\"cuda\" if(torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that the device is specified, we will move our instantiated model to that device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move the instantiated model to the best available device\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function and Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the data acquired and the model specified, the next step if to define the loss function which we wish to minimize\n",
    ">\n",
    "> Since the model returns the log-softmax, we will use the NLL loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will use the NLL loss function since the model returns the log-softmax vector\n",
    "lossFunction = torch.nn.NLLLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, we will define the optimizer algorithm which defines how we will take steps to minimize the loss function\n",
    ">\n",
    "> Our optimizer will be the Adam optimizer\n",
    ">\n",
    "> For this optimizer, a hyperparameter we need to specify is the learning rate, which is essentially the step-size for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the hyperparameter for the learning rate (step size)\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Use the Adam optimizer as the optimizer for minimizing the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonVirtualEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
