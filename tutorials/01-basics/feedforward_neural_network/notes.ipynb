{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network\n",
    "> Building and training of the feed-forward neural network\n",
    ">\n",
    "> **Table of Contents**\n",
    ">> 1. Import Libraries\n",
    ">>\n",
    ">> 2. Configure Device\n",
    ">>\n",
    ">> 3. Load Dataset\n",
    ">>\n",
    ">> 4. Define Model\n",
    ">>\n",
    ">> 5. Define Loss Function and Optimizer\n",
    ">>\n",
    ">> 6. Train Model\n",
    ">>\n",
    ">> 7. Test Model\n",
    ">>\n",
    ">> 8. Save Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before beginning, we must first import all libraries needed for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch for machine learning capabilities\n",
    "import torch\n",
    "\n",
    "#import torchvision for image dataset (MNIST) and image transformations\n",
    "import torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since we plan to use a neural network, we can use the CPU (via CUDA) if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the default device to be the CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#If CUDA is available...\n",
    "if(torch.cuda.is_available()):\n",
    "    #...utillize the GPU by setting the device to CUDA\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we must download and make a reference to the training and testing data from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and make a reference to the training data\n",
    "training_dataset = torchvision.datasets.MNIST(root=\"./data\",\n",
    "                                              train=True,\n",
    "                                              transform=torchvision.transforms.ToTensor(),\n",
    "                                              download=True)\n",
    "\n",
    "#Download and make a reference to the testing data\n",
    "testing_dataset = torchvision.datasets.MNIST(root=\"./data\",\n",
    "                                              train=False,\n",
    "                                              transform=torchvision.transforms.ToTensor(),\n",
    "                                              download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After downloading and referencing the data, we must specify the input pipeline via the DataLoader\n",
    ">\n",
    "> As part of the specification of loading the data, we will need to define the hyperparameter for the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the hyperparameter for the batch size\n",
    "#Batch size: the amount of data points to look through before making an update of the model parameters\n",
    "batch_size = 100\n",
    "\n",
    "#Instantiate a DataLoader object to specify how to load the training data\n",
    "#Shuffling the data and not dropping the last batch (last batch size <= batch size)\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=False)\n",
    "\n",
    "#Instantiate a DataLoader object to specify how to load the testing data\n",
    "#Not shuffling the data and not dropping the last batch (last batch size <= batch size)\n",
    "testing_loader = torch.utils.data.DataLoader(dataset=testing_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             drop_last=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the data formatted by the DataLoader object, the next step is the define our model\n",
    ">\n",
    "> We will first need to build our feed-forward neural network class by combining modules together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crete a neural network class, derived from the torch.nn.Module class\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    #In the constructor, specify the neural network architecture by integers in an iterable object\n",
    "    def __init__(self, layers_nn):\n",
    "        #First, we need to call the constructor of the parent class for the current instance of the class\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #Then define a list of modules that can be appended to\n",
    "        self.module_list = list()\n",
    "        #Then iterate though the number of nodes in each layer, connecting them together alongside using a ReLU non-linear function\n",
    "        #Only perform pattern up to second to last layer\n",
    "        for i in range(len(layers_nn)-2):\n",
    "            #Connect one layer to the next\n",
    "            self.module_list.append(torch.nn.Linear(layers_nn[i], layers_nn[i+1]))\n",
    "            #And then apply a ReLU activation function\n",
    "            self.module_list.append(torch.nn.ReLU())\n",
    "        #For the final layer, we will only map the second to last layer to the last (no ReLU function needed as it will be replaced by softmax)\n",
    "        self.module_list.append(torch.nn.Linear(layers_nn[-2], layers_nn[-1]))\n",
    "        #Log-Softmax used for numberical stability\n",
    "        self.module_list.append(torch.nn.LogSoftmax(dim=1))\n",
    "        #Lastly, convert the list object of modules into a ModuleList object (used so that PyTorch can detect the modules/parameters contained in the list)\n",
    "        self.layers = torch.nn.ModuleList(self.module_list)\n",
    "    \n",
    "    #Define how to apply forwrd propagation to the neural network\n",
    "    def forard(self, x):\n",
    "        #Define the output of each layer which will be propogated from one layer to the next\n",
    "        out = x\n",
    "        #Iterate through each layer, propogating the output as we go\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        #Specify for the output to require the use the gradient for backward propogation\n",
    "        #out.require_grad_()\n",
    "        #return the output\n",
    "        return(out)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we have defined the neural network class, we can instantiate the class to define our model\n",
    ">\n",
    "> We will need to specify the layer sizes as a hyperprameter for our neural network model. This includes the input size, hidden layers, and num classes (output size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the hyperparameter for the neural network layer sizes\n",
    "input_size = 28*28 #For 28x28 image\n",
    "num_classes = 10 #For decision between 10 digits (0-9)\n",
    "layers = [input_size, 500, 100, num_classes] #2 hidden layers\n",
    "\n",
    "#We will then instantiate the model\n",
    "model = NeuralNetwork(layers)\n",
    "\n",
    "#Last, we will send the model to the appropriate device (CPU or GPU (via CUDA))\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the model defined, we must now define the Loss function and optimizer to support the model in learning form data\n",
    ">\n",
    "> First will be the Loss function, which is dependendent on our implementation\n",
    ">\n",
    "> Since we are applying the logSoftmax as the last layer of the model, our loss function will be the Negative Log Likelihood (NLL) Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function to use when training the model\n",
    "lossFunction = torch.nn.NLLLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After defining the loss function, we will then define the optimizer to update the weights\n",
    ">\n",
    "> Our optimizer will use the Adam (adaptive moment estimation) optimizer, which is an extension of stochastic graient descent, combining the benefits of Momentum (moving quickly toward general region of minimum) and RMSprop (perform a more direct movement toward to minimum) (https://www.youtube.com/watch?v=Syom0iwanHo & https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/)\n",
    ">\n",
    "> For this optimizer, the main hyperparameter that needs to be specified is the learning rate (step size) while all others are left at their recommended/default values (https://www.youtube.com/watch?v=JXQT_vxqwIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the hyperparameter for the learning rate (step size)\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Define the optimizer to use when training this model (Adaptive Moment Estimation (Adam) optimizer)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonVirtualEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
