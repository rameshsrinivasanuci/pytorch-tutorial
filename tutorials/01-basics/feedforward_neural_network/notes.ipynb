{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network\n",
    "> Building and training of the feed-forward neural network\n",
    ">\n",
    "> **Table of Contents**\n",
    ">> 1. Import Libraries\n",
    ">>\n",
    ">> 2. Load Dataset\n",
    ">>\n",
    ">> 3. Define Model\n",
    ">>\n",
    ">> 4. Configure Device to Execute Model\n",
    ">>\n",
    ">> 5. Define Loss Function and Optimizer\n",
    ">>\n",
    ">> 6. Train Model\n",
    ">>\n",
    ">> 7. Test Model\n",
    ">>\n",
    ">> 8. Save Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before beginning, we must first import all libraries needed for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch for machine learning capabilities\n",
    "import torch\n",
    "\n",
    "#import torchvision for image dataset (MNIST) and image transformations\n",
    "import torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we must download and make a reference to the training and testing data from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and make a reference to the training data\n",
    "training_dataset = torchvision.datasets.MNIST(root=\"./data\",\n",
    "                                              train=True,\n",
    "                                              transform=torchvision.transforms.ToTensor(),\n",
    "                                              download=True)\n",
    "\n",
    "#Download and make a reference to the testing data\n",
    "testing_dataset = torchvision.datasets.MNIST(root=\"./data\",\n",
    "                                              train=False,\n",
    "                                              transform=torchvision.transforms.ToTensor(),\n",
    "                                              download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After downloading and referencing the data, we must specify the input pipeline via the DataLoader\n",
    ">\n",
    "> As part of the specification of loading the data, we will need to define the hyperparameter for the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the hyperparameter for the batch size\n",
    "#Batch size: the amount of data points to look through before making an update of the model parameters\n",
    "batch_size = 100\n",
    "\n",
    "#Instantiate a DataLoader object to specify how to load the training data\n",
    "#Shuffling the data and not dropping the last batch (last batch size <= batch size)\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=False)\n",
    "\n",
    "#Instantiate a DataLoader object to specify how to load the testing data\n",
    "#Not shuffling the data and not dropping the last batch (last batch size <= batch size)\n",
    "testing_loader = torch.utils.data.DataLoader(dataset=testing_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             drop_last=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the data formatted by the DataLoader object, the next step is the define our model\n",
    ">\n",
    "> We will first need to build our feed-forward neural network class by combining modules together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crete a neural network class, derived from the torch.nn.Module class\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    #In the constructor, specify the neural network architecture by integers in an iterable object\n",
    "    def __init__(self, layers_nn):\n",
    "        #First, we need to call the constructor of the parent class for the current instance of the class\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #Then define a list of modules that can be appended to\n",
    "        self.module_list = list()\n",
    "        #Then iterate though the number of nodes in each layer, connecting them together alongside using a ReLU non-linear function\n",
    "        #Only perform pattern up to second to last layer\n",
    "        for i in range(len(layers_nn)-2):\n",
    "            #Connect one layer to the next\n",
    "            self.module_list.append(torch.nn.Linear(layers_nn[i], layers_nn[i+1]))\n",
    "            #And then apply a ReLU activation function\n",
    "            self.module_list.append(torch.nn.ReLU())\n",
    "        #For the final layer, we will only map the second to last layer to the last (no ReLU function needed as it will be replaced by softmax)\n",
    "        self.module_list.append(torch.nn.Linear(layers_nn[-2], layers_nn[-1]))\n",
    "        #Log-Softmax used for numberical stability\n",
    "        self.module_list.append(torch.nn.LogSoftmax(dim=1))\n",
    "        #Lastly, convert the list object of modules into a ModuleList object (used so that PyTorch can detect the modules/parameters contained in the list)\n",
    "        self.layers = torch.nn.ModuleList(self.module_list)\n",
    "    \n",
    "    #Define how to apply forwrd propagation to the neural network\n",
    "    def forward(self, x):\n",
    "        #Define the output of each layer which will be propogated from one layer to the next\n",
    "        out = x\n",
    "        #Iterate through each layer, propogating the output as we go\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        #Specify for the output to require the use the gradient for backward propogation\n",
    "        #out.require_grad_()\n",
    "        #return the output\n",
    "        return(out)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we have defined the neural network class, we can instantiate the class to define our model\n",
    ">\n",
    "> We will need to specify the layer sizes as a hyperprameter for our neural network model. This includes the input size, hidden layers, and num classes (output size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the hyperparameter for the neural network layer sizes\n",
    "input_size = 28*28 #For 28x28 image\n",
    "num_classes = 10 #For decision between 10 digits (0-9)\n",
    "layers = [input_size, 500, 100, num_classes] #2 hidden layers\n",
    "\n",
    "#We will then instantiate the model\n",
    "model = NeuralNetwork(layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Device to Execute Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since we are using a neural network, we select whether we use the CPU or GPU (via CUDA if it is available)\n",
    ">\n",
    "> The fist step is to determine which device to use. This will be determined by having it on the CPU by default, but use the GPU (via CUDA) if it is avilable\n",
    ">\n",
    "> **What is CUDA?**\n",
    ">> CUDA (Compute Unified Device Architecture) is a package of libraries and software for Machine Learning (ML), Deep Learning (DL), and High Performance Computing (HPC) applications. CUDA enables interfacing directly with the GPU via its own APIs and compiler. Communication with the GPU is essential for ML, DL, and HPC applications because the many cores present on the GPU allow for parallel computation, which is useful when performing matrix arithmetic\n",
    ">>\n",
    ">> In this sense, CUDA can be thought of as libraries with their own systems software for compilation into code that can efficiently talk to the GPU\n",
    ">>\n",
    ">> The core of CUDA is written in C++; however, wrappers for the core C++ code enable use of other programming languages such as Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the default device to be the CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#If CUDA is available...\n",
    "if(torch.cuda.is_available()):\n",
    "    #...utillize the GPU by setting the device to CUDA\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that device has been selected, we can configure the model to use that device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the model to use the resources of the specified device (CPU or GPU (via CUDA))\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the model defined, we must now define the Loss function and optimizer to support the model in learning form data\n",
    ">\n",
    "> First will be the Loss function, which is dependendent on our implementation\n",
    ">\n",
    "> Since we are applying the logSoftmax as the last layer of the model, our loss function will be the Negative Log Likelihood (NLL) Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function to use when training the model\n",
    "lossFunction = torch.nn.NLLLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After defining the loss function, we will then define the optimizer to update the weights\n",
    ">\n",
    "> Our optimizer will use the Adam (adaptive moment estimation) optimizer, which is an extension of stochastic graient descent, combining the benefits of Momentum (moving quickly toward general region of minimum) and RMSprop (perform a more direct movement toward to minimum) (https://www.youtube.com/watch?v=Syom0iwanHo & https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/)\n",
    ">\n",
    "> For this optimizer, the main hyperparameter that needs to be specified is the learning rate (step size) while all others are left at their recommended/default values (https://www.youtube.com/watch?v=JXQT_vxqwIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the hyperparameter for the learning rate (step size)\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Define the optimizer to use when training this model (Adaptive Moment Estimation (Adam) optimizer)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before we begin training the model, one last step is to define the final hyperparameter: the number of epochs which specify the amount of times to look through the training data before we stop training the model (i.e., updating the model parameters)\n",
    ">\n",
    "> After this, we now have the data loaded, the model defined, and all hyperparameters specified; so the last thing to do it to train the model on our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3002\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3072\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2662\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2975\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1537\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1433\n",
      "Epoch [2/5], Step [100/600], Loss: 0.2455\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0337\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1128\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1420\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0640\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0612\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0295\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0888\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1330\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0888\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0898\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1092\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0884\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0600\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0283\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0196\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0406\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0636\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0400\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0371\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0672\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0212\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0711\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0087\n"
     ]
    }
   ],
   "source": [
    "#Define num_epochs hyperparameter (i.e., the number of times to loop through the training data for the training process)\n",
    "num_epochs = 5\n",
    "\n",
    "#Now to begin the training process\n",
    "#Loop through the training data for as many epochs as we specified\n",
    "for epoch in range(num_epochs):\n",
    "    #Loop through each batch\n",
    "    for i, (images, labels) in enumerate(training_loader):\n",
    "        #Reshape images in th batch into a feature matrix (image pixel data on one row and different images on different rows)\n",
    "        images = images.reshape(-1, 28*28) #last batch size can be <= batch_size so -1 is a variable number of rows; 28*28 columns for 28x28 pixels in image\n",
    "        #Move the images tensor to the same device that the model is configured to\n",
    "        images = images.to(device)\n",
    "        #Move the labels tensor to the same device that the model is configured to\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Perform forward propogation on the model from the data in the current batch\n",
    "        outputs = model(images)\n",
    "\n",
    "        #Compute the current loss of the model\n",
    "        loss = lossFunction(outputs, labels)\n",
    "\n",
    "        #Reset the gradient to prepare for backards propogation\n",
    "        optimizer.zero_grad()\n",
    "        #Perform backards propogation to get the gradient vector\n",
    "        loss.backward()\n",
    "        #Take a step corresponding to the negative of the gradient using the specified optimizer algorithm (Adam: Adaptive Movement)\n",
    "        optimizer.step()\n",
    "\n",
    "        #On each 100 batches...\n",
    "        if((i+1)%100 == 0):\n",
    "            #print the current epoch, batch (step), and current model loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(training_loader)}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the model no properly trained, we can test the model's performance on testing data (i.e., data that have never been seen/learned by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10,000 test images: 98.06%\n"
     ]
    }
   ],
   "source": [
    "#For memory efficeincy, perform computations without computing the gradient\n",
    "with torch.no_grad():\n",
    "    #Initiallize counters for the total tests and total correct\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    #Loop through each batch in the testing dataset\n",
    "    for images, labels in testing_loader:\n",
    "        #Reshape the images in the batch to create a feature matrix (with a variable number of rows)\n",
    "        images = images.reshape(-1, 28*28)\n",
    "\n",
    "        #Move the images tensor to the device configured for the model\n",
    "        images = images.to(device)\n",
    "        #Move the labels tensor to the device configured for the model\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Perform a forward propogation of the testing data on the model\n",
    "        outputs = model(images)\n",
    "\n",
    "        #Get the probability values and the class predictions (that contain the highest probability) for each image\n",
    "        probability_values, class_predictions = torch.max(outputs.data, dim=1)\n",
    "\n",
    "        #Add the total number of predictions made on this batch\n",
    "        total += labels.size(0)\n",
    "        #Add the total number of correct predictions made on this batch\n",
    "        correct += (class_predictions == labels).sum().item()\n",
    "\n",
    "    #Print the total accuracy of the trained model on the testing data\n",
    "    print(f\"Accuracy on 10,000 test images: {100*correct/total}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, if we are satisfied with the accuracy of the model on the testing data, we can save the trained model for reloading and use on other machines/applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model to a checkpoint file\n",
    "torch.save(model.state_dict(), \"model.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonVirtualEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
