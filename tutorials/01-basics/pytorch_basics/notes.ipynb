{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics\n",
    "> Introduction into the basics of using the PyTorch tools alongside NumPy\n",
    ">\n",
    "> **Table Of Contents:**\n",
    ">> 1. Using AutoGrad for Stochastic Gradient Descent\n",
    ">>\n",
    ">> 2. Converting Data Between PyTorch And NumPy\n",
    ">>\n",
    ">> 3. The Input Pipeline\n",
    ">>\n",
    ">> 4. Running a Pre-Trained Model\n",
    ">>\n",
    ">> 5. Saving and Loading Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AutoGrad for Stochastic Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is AutoGrad?**\n",
    ">> AutoGrad (automatic gradient) is the automatic differentition tool, recording the change history of tensors to follow the changes and apply the chain rule to compute gradients (https://pytorch.org/docs/stable/notes/autograd)\n",
    ">\n",
    "> **Why is the gradient important?**\n",
    ">> As written by Goodfellow et al. (ch. 4.3), deep-learning involves the optimization of an objective function. And so, the gradient allows us to get the direction with the highest rate of change at a specifc point. By then taking the opposite direction (i.e., gradient descent), we can minimize the objective function (also known as the loss function) which results in a relatively good set of parameters.\n",
    ">\n",
    "> **How to import AutoGrad?**\n",
    ">> AutoGrad is apart of the torch module (torch.AutoGrad), so we only need to import torch to use the AutoGrad tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the torch module to use AutoGrad\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How to use AutoGrad?**\n",
    ">> As the name implies, AutoGrad works automatically in the background; however, we must specify when we want it to be keeping track of tensor change history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1.], requires_grad=True)\n",
      "w = tensor([2.], requires_grad=True)\n",
      "b = tensor([3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Create tensors specifying the gradient will be needed for later\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "#Print the current tensors made\n",
    "print(f\"x = {x}\")\n",
    "print(f\"w = {w}\")\n",
    "print(f\"b = {b}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**In what way do we \"change\" the tensors?**\n",
    ">> We can apply forward propogation by using mathematical functions such as a linear function to combine our tensors and get a resulting tensor. This works out well because an artificial neural network is nothing more than a mathematical function, mapping our input to the output\n",
    ">\n",
    "> **What is Forward Propogation?**\n",
    ">> Forward propogation is the process of feeding all of our input data into a model to get the resulting output (i.e., get the prediction made by a model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply forward propogation via a simple linear equation and save the result\n",
    "y = w*x + b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **We have some history of changes made; now what?**\n",
    ">> After applying forward propogation and now having some history for the output tensor, we can determine the gradient by using backwards propogation\n",
    ">\n",
    ">**What is backwards propogation?**\n",
    ">> Backwards propogation the process of comparing the output data to the correct label and having that error self-correct the parameters in a model by working backwards. A subset of the training data (batch) is used to calculate a vector which is proportional to the negative gradient, allowing us to take a single gradient descent step to correct the parameter values (i.e., lessen the error of the model on the training data) (https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    ">\n",
    "> **What is Stochastic Gradient Descent?**\n",
    ">> The repeated process of using backwards propogation until a certain number of passes through the entire dataset (i.e., epochs) are made is called Stochatic Gradient Descent (https://www.youtube.com/watch?v=Ilg3gGewQ5U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform backwards propogation from the y tensor\n",
    "y.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How can we think of the resulting gradient from backpropogation in a multi-dimensional space?**\n",
    ">> We can think of the gradient in terms of its components. Each component in the gradient tells how sensitive that parameter is changes in its value. Another way to put it is that the value of the component in the gradient is the multiplied weight given to the input that then show up in the output. An example is seen below with x being the only component with 2. The reason is because it is being multiplied by the weight (which is 2) an the bias term does not effect it because it is an added constant. So if we double the x value, the output would be 4 times as large. (https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x component of the gradient: tensor([2.])\n",
      "w component of the gradient: tensor([1.])\n",
      "b component of the gradient: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "#Print the component of the gradient after performing backward propogation\n",
    "print(f\"x component of the gradient: {x.grad}\")\n",
    "print(f\"w component of the gradient: {w.grad}\")\n",
    "print(f\"b component of the gradient: {b.grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What does this look like in a more complex example?**\n",
    ">> We will look through the same example as above, but we will instead rely more on the built in functions and modules the PyTorch offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing model parameter values:\n",
      "w: Parameter containing:\n",
      "tensor([[-0.4801,  0.0398, -0.5144],\n",
      "        [ 0.2996, -0.0121,  0.4330]], requires_grad=True)\n",
      "b: Parameter containing:\n",
      "tensor([-0.0180,  0.2725], requires_grad=True)\n",
      "\n",
      "Current model loss: 0.7636953592300415\n",
      "\n",
      "Printing gradient of first backward propogation:\n",
      "dL/dw: tensor([[-0.6175, -0.5759, -0.5153],\n",
      "        [ 0.1304,  0.0595,  0.0957]])\n",
      "dL/db: tensor([-1.1256,  0.1807])\n",
      "\n",
      "Loss after 1 Stochastic Gradient Descent step: 0.740820050239563\n"
     ]
    }
   ],
   "source": [
    "#First import the necessary module\n",
    "import torch\n",
    "\n",
    "#Next create a toy dataset (random tensors)\n",
    "x = torch.rand(10, 3) #A set of 10 feature vectors with 3 features each\n",
    "y = torch.rand(10, 2) #A set of 10 label vectors with 2 labels each\n",
    "\n",
    "#Create a linear model by creating a fully connected 2-layer neural network\n",
    "input_layer = 3 #3 input nodes\n",
    "output_layer = 2 #2 output nodes\n",
    "linearModel = torch.nn.Linear(input_layer, output_layer)\n",
    "\n",
    "#Print the current values of the weights & biases for the linear model\n",
    "print(\"Printing model parameter values:\")\n",
    "print(f\"w: {linearModel.weight}\")\n",
    "print(f\"b: {linearModel.bias}\\n\")\n",
    "\n",
    "#With the model defined, we can now apply forward propogation to get the current model predictions\n",
    "predictions = linearModel(x)\n",
    "\n",
    "#With the model's predictions saved, we can now compare them to the true labels and get the total loss for the model\n",
    "lossFunction = torch.nn.MSELoss() #First create an MSE Loss object to act as the loss function\n",
    "loss = lossFunction(predictions, y) #Then compute the loss using the Mean-Square-Error (MSE)\n",
    "\n",
    "#Print the current loss of the model\n",
    "print(f\"Current model loss: {loss}\\n\")\n",
    "\n",
    "#With the loss calculated, we can minimize it using backward propogation\n",
    "loss.backward()\n",
    "\n",
    "#Print the gradients that were computed using backwards propogation\n",
    "print(\"Printing gradient of first backward propogation:\")\n",
    "print(f\"dL/dw: {linearModel.weight.grad}\")\n",
    "print(f\"dL/db: {linearModel.bias.grad}\\n\")\n",
    "\n",
    "#Since the result of backwards propogation is proportional to the negative gradient, it tells us the exact direction to take a step in our parameter space\n",
    "step_size = 0.01\n",
    "linearModel.weight.data -= step_size * linearModel.weight.grad.data\n",
    "linearModel.bias.data -= step_size * linearModel.bias.grad.data\n",
    "\n",
    "#With the weights updated, we can recompute the loss using the improved parameters\n",
    "predictions = linearModel(x)\n",
    "loss = lossFunction(predictions, y)\n",
    "print(f\"Loss after 1 Stochastic Gradient Descent step: {loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Is there a simpler way to perform a single stochastic gradient descent (SGD) step?**\n",
    ">> There is only one thing that can be added, an optimizer object. There is a SGD object than can perform the update to the parameters when the gradient is computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Since the result of backwards propogation is proportional to the negative gradient, it tells us the exact direction to take a step in our parameter space\n",
    "step_size = 0.01\n",
    "linearModel.weight.data -= step_size * linearModel.weight.grad.data\n",
    "linearModel.bias.data -= step_size * linearModel.bias.grad.data\n",
    "\"\"\"\n",
    "#=========================\n",
    "#Replace the code above with the code below to utillize the SGD optimizer\n",
    "#=========================\n",
    "\n",
    "#Define an optimizer for the model parameters Using Stochatic Gradient Descent (SGD))\n",
    "learning_rate = 0.01 #Set how quickly the model will learn (i.e., the weight multiplied to the change on the parameters)\n",
    "optimizer = torch.optim.SGD(linearModel.parameters(), lr=learning_rate)\n",
    "\n",
    "#Perform a stingle step of the SGD optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Data Between PyTorch And NumPy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How can we input data to use in PyTorch?**\n",
    ">> To convert data into PyTorch tensors, there is an easy conversion function that converts NumPy arrays into PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "y:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#import necessary modules\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Create a python list\n",
    "data = [\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "]\n",
    "\n",
    "#Convert python list into a numpy array\n",
    "x = np.array(data)\n",
    "print(f\"x:\\n{x}\\n\")\n",
    "\n",
    "#Convert the numpy array to a pytorch tensor\n",
    "y = torch.from_numpy(x)\n",
    "print(f\"y:\\n{y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Can I convert the PyTorch tensor back into a NumPy array?**\n",
    ">> Yes! The conversion from PyTorch tensors to NumPy arrays is easy, only requiring one function for each type of conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z:\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "#Convert the pytorch tensor into a numpy array\n",
    "z = y.numpy()\n",
    "print(f\"z:\\n{z}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is the sequence of the Input Pipeline?**\n",
    ">> 1. Download dataset\n",
    ">>\n",
    ">> 2. Read first data vector to ensure the data was downloaded properly\n",
    ">>\n",
    ">> 3. Use the prebuilt PyTorch DataLoader for queues and threads\n",
    ">>\n",
    ">> 4. Iterate through the data an train the model\n",
    ">\n",
    "> **How to download a dataset?**\n",
    ">> We simply need to get the data on our local storage. There are some references to common datasets which PyTorch has functions for downloaing them\n",
    ">\n",
    "> **What happends if I already downloaded the dataset?**\n",
    ">> As long as the root path is the same, the PyTorch should be able to detect that the files already exist and prevent a redownload of the data (https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Import torchvision module for image dataset and image to tensor transformation\n",
    "import torchvision\n",
    "\n",
    "#Download and construct the CIFAR-10 image dataset\n",
    "training_dataset = torchvision.datasets.CIFAR10(root=\".\", \n",
    "                                                train=True, \n",
    "                                                transform=torchvision.transforms.ToTensor(),\n",
    "                                                download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How to read the data?**\n",
    ">> PyTorch both downloads and constructs the data. In that case, the data is saved as a variable and can be read by normal inexing operations. One thing to keep in mind is that the data in each element is separated into the features and label via a tuple. This allows for the two of them to be easily separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size of image 0: torch.Size([3, 32, 32])\n",
      "Label of image 0: 6\n"
     ]
    }
   ],
   "source": [
    "#Read the first data vector in the dataset\n",
    "image_tensor, label = training_dataset[0]\n",
    "\n",
    "#Print the size of the image tensor\n",
    "print(f\"Tensor size of image 0: {image_tensor.size()}\") #can also use tensor.shape to get the size\n",
    "print(f\"Label of image 0: {label}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How to use the prebuilt DataLoader?**\n",
    ">> The dataloader is a class that allows for queues and threads of the data to be made in a very simply way. To use it, we simply need to provide our constructed dataset and some other parameters such as batch size and whether or not to shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataloader object from the CIFAR-10 dataset with a batch size of 64 and shuffle the order of the data\n",
    "training_dataloader = torch.utils.data.DataLoader(dataset=training_dataset,\n",
    "                                                  batch_size=64,\n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How to iterate through the data**\n",
    ">> This can be done using a simple for loop over the dataloader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a for loop to iterate over each data vector in the dataloader\n",
    "for images, labels in training_dataloader:\n",
    "    #In the for loop, we will put code for our model to perform on each image/label in the dataset\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How can we create our own dataset that is compatable with the dataloader?**\n",
    ">> We can create a custom class which inherits the dataset class used by PyTorch. In this derived class, we will then need to override a few member functions to have them be more suited for our new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template for own custom dataset class below\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    #Define the constructor\n",
    "    def __init__(self):\n",
    "        #We will need to initiallize the filepath/list of files for our dataset\n",
    "        pass\n",
    "    #Define what to do for the indexing operator\n",
    "    def __getitem__(self, index):\n",
    "        #We will need to read the data from the file, proprocess the data, and then return the (feature, label) tuple\n",
    "        pass\n",
    "    #Define the length of the dataset\n",
    "    def __len__(self):\n",
    "        #replace the 999 with the actual length of the dataset\n",
    "        return 999"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Once we create a custom dataset, how do we use it with the dataloader?**\n",
    ">> To use the custom dataset, we can simply instantiate the class and use it as a built-in dataset such as the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the custom dataset\n",
    "custom_dataset = CustomDataset()\n",
    "\n",
    "#now use the dataset with the dataloader as you would with any other dataset\n",
    "training_dataloader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                                  batch_size=64,\n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Pre-Trained Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How to download and load a pre-trained model?**\n",
    ">> PyTorch has some models built in to use with a specific parameter to indicate if you want to model to be pre-trained or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mike5\\OneDrive\\Desktop\\PSYCH199\\PSYCH199-Code\\PythonVirtualEnvironment\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mike5\\OneDrive\\Desktop\\PSYCH199\\PSYCH199-Code\\PythonVirtualEnvironment\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\mike5/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "#Import the torchvision module for the resnet image model\n",
    "import torchvision\n",
    "\n",
    "#Download and load the resnet18 pre-trained model\n",
    "resnet = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonVirtualEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
